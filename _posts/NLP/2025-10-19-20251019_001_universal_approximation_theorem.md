---
title: "[자연언어처리] 1-2. 만능 머신러닝 모델은 존재할까 Universal Approximation Theorem" # 제목 (필수)
excerpt: 최신 모델만이 아닌, 여러 가지 모델을 공부해야 하는 이유 # 서브 타이틀이자 meta description (필수)
date: 2025-10-19 00:25:00 +0900      # 작성일 (필수)
lastmod: 2025-10-19 00:25:00 +0900   # 최종 수정일 (필수)
last_modified_at: 2025-10-19 00:25:00 +0900   # 최종 수정일 (필수)
categories: [NLP, deep_learning, AI]       # 다수 카테고리에 포함 가능 (필수)
tags: nlp 자연언어처리 자연어 자연언어 ai universal approximation theorem inductive learning bias 만능 머신러닝 모델 machinelearning machine learning 귀납편향 학습편향                       # 태그 복수개 가능 (필수)
classes:         # wide : 넓은 레이아웃 / 빈칸 : 기본 //// wide 시에는 sticky toc 불가
toc: true        # 목차 표시 여부
toc_label:       # toc 제목
toc_sticky: true # 이동하는 목차 표시 여부 (toc:true 필요) // wide 시에는 sticky toc 불가
header: 
  image:         # 헤더 이미지 (asset내 혹은 url)
  teaser:        # 티저 이미지??
  overlay_image: /assets/images/banners/banner.gif            # 헤더 이미지 (제목과 겹치게)
  # overlay_color: '#333'            # 헤더 배경색 (제목과 겹치게) #333 : 짙은 회색 (필수)
  video:
    id:                      # 영상 ID (URL 뒷부분)
    provider:                # youtube, vimeo 등
sitemap :                    # 구글 크롤링
  changefreq : daily         # 구글 크롤링
  priority : 1.0             # 구글 크롤링
author: # 주인 외 작성자 표기 필요시
---
<!--postNo: 20251019_001-->  

## 보편 근사 정리 Universal Approximation Theorem  

### 이름 뜻 풀이  

- Universal : 보편적인, 모든 것에 적용되는, 전 세계적인  
- Approximation : 근사, 근사값, 정확한 값에 가까운 값  
- Theorem : 정리, 증명된 명제.  
- 의역을 보태면 **보편적으로 모든 문제에 대해 근사할 수 있는 능력에 관한 정리** 가 된다.  

### 정의  

- 인공 신경망의 기본적인 능력을 설명하는 이론으로, MLP의 너비와 깊이를 충분히 들리면, 어떠한 연속 함수도 표현이 가능하다라는 정리이다.  
- 쉽게 말해, **충분히 큰 인공 신경망으로 현실 세계의 모든 복잡한 문제를 근사(비슷)하게 설명(또는 모방)할 수 있다**는 것이다.  
- 단, 문제에 맞는 충분히 큰 모델 등, 일정 조건들이 필요하다.  

> MLP : multi-layer perceptron - 다층 퍼셉트론  

### 현실에서 보편 근사 모델은?  

- 현실 세계에서는 각 문제 분야에 맞는 특정 모델들이 사용되는 것을 볼 수 있다.  
- 예를 들어 이미지쪽은 CNN, 자연어는 Transformer 구조의 모델을 사용한다.  
- MLP 가 모든 모델의 상위 개념이라면 MLP만 쓰겠지만, 현실적으로는 그러하지 않은 것이다.  
- 그 이유는, 각각의 서로 다른 모델들이 가지는 제약과 장점들이 다르기 때문이다.  

|예시|설명|
|---|---|
|선형회귀 모델|- 금융권에서 많이 사용된다.<br>데이터에 선형적 관계가 있어야 한다는 제약이 있다.<br>대신, 모델의 결과를 해석하기 쉽다는 장점이 있기 때문에 사용된다.|


## 귀납 편향 Inductive Bias  

### 이름 뜻 풀이  

- Inductive : 귀납적인(사례들에서 일반화된 원리나 패턴을 이끌어내는 추론)  
- Bias : 편향, 편견, 성향, 치우침  

### 정의  

- 모델이 데이터를 학습하기 전에, 알고리즘 설계 시 모델의 구조에 미리 주입된 가정이나 제약 조건 또는 힌트  
- 머신러닝 모델의 학습을 **특정 방향으로 유도**하는 역할을 한다.  
- learning bias, 학습 편향이라고도 부른다.  
- 머신러닝은 사례(=데이터)들로부터 학습을 한다. -> 때문에 귀납적이라고 지칭  

### 목적과 영향  

- 제한된 데이터만으로도 현실의 복잡한 규칙을 어느 정도 파악할 수 있도록  
- 이러한 편향은 **모델이 특정 문제에 대한 해결력이나 특징 또는 제약**을 가지게 한다.  

### 예시  

|모델|가정(귀납 편향)|
|---|---|
|선형 회귀 모델|변수 X, Y 사이에 선형적 관계가 있을 것이다.<br>오컴의 면도날 원칙에 기초|
|KNN|가까운 위치에 있는 두 데이터의 클래스는 비슷할 것이다.|
|GBT<br>의사결정나무|개별 변수들을 순차적으로 고려하면 최종적 판단이 가능하다.<br>=변수들의 순차적인 조합이 현상을 나타낸다.|
|CNN|이미지의 한 픽셀은 멀리 떨어진 픽셀보다<br>가까운 주변 픽셀과 밀접한 관련이 있을 것이다.|

### 귀납 편향과 보편 근사 모델  

- 즉, 모델은 **특정한 가정**을 가지며, 이 가정과 잘 들어맞지 않는 문제는 해당 모델로 해결이 어렵다.  
- 반대로, 모델이 가진 가정이 성립하는 데이터가 주어졌을 때, 모델은 좋은 성능을 발휘할 수 있다.  
- 이 특정 가정이 바로 Inductive Bias 이다.  
- 예를 들어, 이미지에 대해 GBT는 과적합 확률이 높지만, 구조가 깊은 CNN은 효과적으로 이미지를 처리할 수 있다.  
- 때문에 개발자는 자신이 사용하려는 모델의 가정을 잘 알아야 효과적인 모델을 만들 수 있다.    

### CNN과 Transformer의 예시  

- AI 분야의 혁신은, 특정 분야의 문제를 잘 해결할 수 있는 구조를 가진 AI 모델의 등장을 통해 이뤄졌다. 2012년의 CNN, 2017년의 Transformer가 이에 해당된다.  
- 예를 들어 CNN 은 이미지에, Transformer는 텍스트 데이터에 잘 맞는 Inductive Bias가 가미된 모델이다.  
- 다른 예시로 추천 분야에서 많이 사용되는 Matrix Factorization를 들 수 있다.    


## 딥러닝이 만능은 아니다  

### 딥러닝이 모든 분야에 만능은 아니다  

- 딥러닝(보통 CNN 혹은 Transformer) 모델들이 모든 분야의 데이터를 다 잘 학습할 수 있는 건 아니다.  
- 실제로 tabular 데이터에서는 GBT 가 딥러닝 모델에 비해 더 좋은 성능을 보인다.  

### 다양한 모델을 배워야 하는 이유  

- 밭을 갈 때는 쟁기가, 나무를 벨 때에는 도끼가 필요하다. 전기톱이 발명됐다고 해서 전기톱으로 밭을 갈려고 해서는 안된다.  
- 각 모델은 잘하는 분야가 있으며, 꼭 딥러닝 모델이라고 해서 만능은 아니다.  
- 따라서 **주어진 문제와 데이터에 적합한 모델을 선택하기 위해 다양한 모델을 공부해야** 한다.  
- 회사에서 상사가, 혹은 고객이 "딥러닝을 적용하면 되지 않냐"라고 한다면, 위 내용을 염두에 두고 소통해야 한다.  
- 우리는 대세에 휩쓸리는 시대에 살고 있고, 요즘은 딥러닝과 LLM이 대세인 시대이다.  
- 이런 시대에서 딥러닝이 만능이라는 사고의 함정에 빠지지 말고, 문제를 잘 해결할 수 있는 알맞은 도구(모델)을 선택해 사용하는 태도를 지켜나갈 수 있도록 노력해야 한다.  

### 데이터의 중요성  

- 모델은 문제 해결을 위한 절반의 준비물에 지나지 않는다. **다른 절반은 바로 데이터**다.  
- 현실을 설명하기 위한 충분한 정보를 가진 데이터가 있어야만 모델이 의미 있는 결과를 낼 수 있다.  
- 예를 들어 고양이와 강아지를 구분하기 위해서는 고양이와 강아지가 찍힌 사진이 있으면 된다. 사람이 그 사진을 보고 둘을 구분할 수 있기 때문에, 이제 기계적으로 고양이와 강아지를 구분할 수 있는 모델 구조를 찾으면 되는 것이다.  
- 하지만 주가 예측을 위해 지금까지의 주가 데이터만을 준비했다고 가정해보자. 데이터가 충분하지 않을 수 있다. 현실의 주가를 설명하기 위해서는 회사 전략, 세계 정세, 환율 등 영향 요인이 많기 때문이다.  

## 머신러닝이 이루어지기 위한 조건  

### 효과적인 머신러닝을 위해  

- (1) 데이터가 충분한 양의 필요 정보를 담고 있어야 한다.  
- (2) 모델이 데이터에 맞는 가정(구조)를 가지고 있어야 한다.  

### 효과적인 모델 만들기에 노력이 필요한 이유  

- 현실 세계에서 효과적인 모델을 만드는 데에는 많은 노력이 필요하다.  
- 그 이유는, 첫째로 충분한 데이터와 알맞은 모델을 준비해야 하기 때문이다.  
- 둘째로, 잘 안풀렸을 때 데이터와 모델 양쪽에서 문제 원인을 찾고 교차 실험 및 검증 작업이 필요하기 때문이다.  


## Reference  

방송통신대학교 - 자연언어처리 수업 (유찬우 교수)  

