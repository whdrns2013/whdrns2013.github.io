---
title: "[자연언어처리] 2-2. 텍스트 표현의 방법들 text representation" # 제목 (필수)
excerpt: 텍스트를 컴퓨터가 이해하고 처리할 수 있게 변환하는 방법들 # 서브 타이틀이자 meta description (필수)
date: 2025-10-20 00:07:00 +0900      # 작성일 (필수)
lastmod: 2025-10-20 00:07:00 +0900   # 최종 수정일 (필수)
last_modified_at: 2025-10-20 00:07:00 +0900   # 최종 수정일 (필수)
categories: [NLP, deep_learning, AI]       # 다수 카테고리에 포함 가능 (필수)
tags: nlp 자연언어처리 자연어 자연언어 텍스트표현 텍스트 표현 text representation 수치화 방법 방법론 방식 기법 기법들 bag of words bagofwords 단어주머니 단어 주머니 ai 머신러닝 모델 machinelearning machine learning                       # 태그 복수개 가능 (필수)
classes:         # wide : 넓은 레이아웃 / 빈칸 : 기본 //// wide 시에는 sticky toc 불가
toc: true        # 목차 표시 여부
toc_label:       # toc 제목
toc_sticky: true # 이동하는 목차 표시 여부 (toc:true 필요) // wide 시에는 sticky toc 불가
header: 
  image:         # 헤더 이미지 (asset내 혹은 url)
  teaser:        # 티저 이미지??
  overlay_image: /assets/images/banners/banner.gif            # 헤더 이미지 (제목과 겹치게)
  # overlay_color: '#333'            # 헤더 배경색 (제목과 겹치게) #333 : 짙은 회색 (필수)
  video:
    id:                      # 영상 ID (URL 뒷부분)
    provider:                # youtube, vimeo 등
sitemap :                    # 구글 크롤링
  changefreq : daily         # 구글 크롤링
  priority : 1.0             # 구글 크롤링
author: # 주인 외 작성자 표기 필요시
---
<!--postNo: 20251020_001-->  

## 텍스트 표현  

### 텍스트 표현의 정의  

- Text Representation 
- 텍스트를 컴퓨터가 이해하고 처리할 수 있는 수치 형태로 변환하는 것  

### 텍스트 표현의 방식    

#### 텍스트 표현 방식의 종류  

|방법론|영문명칭|설명|예시|
|---|---|---|---|
|등장 횟수 기반 방식|count-based|텍스트에 등장하는 단어의 빈도를 활용하여 벡터로 표현하는 방식|Bag of Words<br>TF-IDF|
|분포 기반 방식|distributed|단어의 의미를 벡터 공간에 표현하는 방식|Word2Vec<br>GloVe<br>FastText|
|신경망 기반 방식|neural network-based|하나의 단어도 문맥에 따라 다른 벡터로 표현|RNN/LSTM/GRU기반<br>Transformer기반|

#### (1) 등장 횟수 기반 표현  

- count-based representation  
- 텍스트에 등장하는 단어의 빈도를 활용해 벡터로 표현하는 방식  
- 단어의 순서는 고려하지 않고, 단어의 출현 유무 또는 횟수에만 집중  

|기법|설명|
|---|---|
|Bag of Words|- 모든 문서에 등장한 모든 단어에 고유 인덱스를 부여하고, 각 문서를 해당 단어들의 등장 횟수로 표현한 벡터를 만든다.<br>- 간단하고 구현이 쉽고, 문서의 주제 파악에 유용하다.<br>- 단어의 순서를 완전히 무시하는 단점<br>- 또한 모든 단어에 대한 벡터를 만드므로 벡터의 차원이 커지고 0이 많은 Sparse Vector 가 되기 쉽다.|
|TF-IDF|- 단순 빈도 뿐만 아니라, 그 단어가 전체 문서 집합에서 얼마나 희귀하게 나타나는지를 함께 고려해 단어의 중요도를 측정한다<br>- 특정 문서에서 중요한 역할을 하는 단어를 잘 부각시켜준다.<br>- 하지만 차원이 커지고 희소행렬이 되기 쉽다는 단점은 같다.|

#### (2) 분포 기반 표현  

- distributed representation  
- 또는 단어 임베딩(Word Embedding)  
- 단어의 의미를 벡터 공간에 표현하는 방식이다.  
- 비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 가진다는 분포 가설(Distributional Hypothesis)에 기반한다.  
- 단어를 밀집 벡터(Dense Vector)로 표현해 희소성 문제를 해결하고 단어의 의미적, 문법적 관계를 반영한다.  

|기법|설명|
|---|---|
|Word2Vec|- 가장 기본이 되는 단어 임베딩 모델<br>- 주변 단어를 통해 중심 단어를 예측하거나(CBOW)<br>- 중심 단어를 통해 주변 단어를 예측하는(Skip-gram)방식으로 학습된다.|
|GloVe|- Global Vectors for Word Representation<br>- 단어의 전역적인 동시 출현 빈도 정보를 활용해<br>- Word2Vec의 단점인 지역적 문제만 고려한다는 점을 보완한 모델|
|FastText|- 단어를 문자 n-gram의 조합으로 표현하여<br>- 오타나 미등록 단어(OOV)문제에 강한 모델<br>- 형태론적 정보를 잘 포착한다.|

#### (3) 신경망 기반 표현  

- Neural Network-based Representation  
- 최근 딥러닝 기반 모델에서 주로 사용하는 방식으로  
- 하나의 단어도 문맥에 따라 다른 벡터로 표현하는 방식이다.  

|기법|설명|
|---|---|
|RNN/LSTM/GRU 기반 임베딩|- 초기 시퀀스 모델에서 사용됨<br>- 단어 벡터를 순차적으로 입력받아 문맥 정보를 반영한 최종 벡터를 만듦|
|Transformer 기반 임베딩|- 어텐션 매커니즘을 사용해 입력 문장 전체를 동시에 고려하며 문맥을 파악함<br>- 문장 내의 동일한 단어라도 문맥에 따라 다른 으미를 반영한 임베딩 벡터를 생성함<br>- 현대 NLP의 표준으로 자리 잡고 있음|

## Reference  

- 검증 필요  