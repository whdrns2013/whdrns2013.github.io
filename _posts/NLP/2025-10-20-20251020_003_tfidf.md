---
title: "[자연언어처리] 2-4. 텍스트 표현 (2) TF-IDF" # 제목 (필수)
excerpt: 단어의 중요성을 측정하는 빈도 기반 텍스트 표현법 TF-IDF # 서브 타이틀이자 meta description (필수)
date: 2025-10-20 01:33:00 +0900      # 작성일 (필수)
lastmod: 2025-10-20 01:33:00 +0900   # 최종 수정일 (필수)
last_modified_at: 2025-10-20 01:33:00 +0900   # 최종 수정일 (필수)
categories: [NLP, deep_learning, AI]       # 다수 카테고리에 포함 가능 (필수)
tags: nlp 자연언어처리 자연어 자연언어 TFIDF TF-IDF TF IDF 용어 문서 역문서 빈도 용어빈도 문서빈도 역문서빈도 ai 머신러닝 모델 machinelearning machine learning                       # 태그 복수개 가능 (필수)
classes:         # wide : 넓은 레이아웃 / 빈칸 : 기본 //// wide 시에는 sticky toc 불가
toc: true        # 목차 표시 여부
toc_label:       # toc 제목
toc_sticky: true # 이동하는 목차 표시 여부 (toc:true 필요) // wide 시에는 sticky toc 불가
header: 
  image:         # 헤더 이미지 (asset내 혹은 url)
  teaser:        # 티저 이미지??
  overlay_image: /assets/images/banners/banner.gif            # 헤더 이미지 (제목과 겹치게)
  # overlay_color: '#333'            # 헤더 배경색 (제목과 겹치게) #333 : 짙은 회색 (필수)
  video:
    id:                      # 영상 ID (URL 뒷부분)
    provider:                # youtube, vimeo 등
sitemap :                    # 구글 크롤링
  changefreq : daily         # 구글 크롤링
  priority : 1.0             # 구글 크롤링
author: # 주인 외 작성자 표기 필요시
---
<!--postNo: 20251020_003-->  

## TF-IDF  

### 정의  

- 단순 빈도 뿐만 아니라, 그 단어가 전체 문서 집합에서 얼마나 희귀하게 나타나는지를 함께 고려해 단어의 중요도를 측정하고, 이를 바탕으로 텍스트를 벡터로 변환하는 것.  

### 만드는 방법  

- TF 와 IDF 의 곱으로 계산한다.  
- TF : Term Frequency(용어 빈도). 문서 내 단어 빈도를 비율로 나타낸 것.  
- DF : Document Frequency(문서 빈도). 특정 단어가 포함된 문서의 빈도를 비율로 나타낸 것. 
- IDF : Inverse Document Frequency(역문서 빈도). DF의 역분. 특정 단어가 포함된 문서의 수가 작을수록 증가한다.  
- 예시 : 단어 A가 B문서에서 다른 단어에 비해 많이 등장하면서, A단어를 포함하는 다른 문서의 수가 적을수록 B 문서에서 A 단어에 대한 TF-IDF 값이 증가한다.  

### 계산식  

#### (1) TF (Term Frequency)  

![](/assets/images/20251020_003_001.png)  

- TF(w,d) : 문서 d의 모든 단어 등장 빈도 중 w의 등장 빈도 비율  
- f(w,d) : 단어 w가 문서 d에 실제로 등장한 횟수 (w의 절대 빈도)  
- 분모항 : 문서 d에서 등장하는 모든 단어의 등장 횟수 합  

#### (2) IDF (Inverse Document Frequency)  

![](/assets/images/20251020_003_002.png)  

- IDF(w) : 단어 w가 전체 문서 집합(corpus) D에서 얼마나 희귀하게 나타나는지의 정도  
- |D| : corpus 안의 전체 문서의 개수  
- f(w,D) : corpus 안에서 단어 w를 포함하는 문서의 개수  
- 희귀한 단어일수록 값이 높아져 중요도가 올라간다.  
- 값의 변별력을 높이기 위해 log를 취한다.    


#### (3) TF-IDF  

![](/assets/images/20251020_003_003.png)  

- TF 와 IDF의 곱  
- 문서 d 안에 단어 w가 여러 번 등장할수록 tf 값이 증가하며  
- corpus 안에 단어 w를 포함하는 문서가 적을수록 idf 값이 증가한다.  
- 반대로 f(w,D) 가 |D|와 비슷해지면(일반적으로 많이 쓰이면) idf 값이 0에 가까워져, 해당 단어가 무시된다.  

### TF-IDF의 특징  

- 문서를 단어들의 빈도로 나타낸다는 점에서 BoW와 유사  
- 하지만 모든 단어를 동등한 비중으로 다루는 게 아니라, 일반적으로 많이 쓰이는 단어일수록 무시하는 페널티를 준다는 점에서 차이가 있음  
- TF-IDF의 전제는, 많은 문서들에 공통적으로 포함된 단어는, 어느 특정 문서에서만 등장하는 단어보다 문서의 고유한 특성을 나타내기에 제공하는 정보가 적다라는 것  

## 실습  

### 실습 데이터  

- gensim 의 샘플 데이터를 활용한다.  
- text corpus 는 9개의 문장으로 이루어져 있다.  
- 전처리 코드 등은 샘플 데이터의 코드 혹은, 이전 bag of words 포스팅을 참고한다.  

[gensim - example data](https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#sphx-glr-auto-examples-core-run-core-concepts-py)  

[jongya's blog - bag of words](https://whdrns2013.github.io/nlp/deep_learning/ai/20251020_002_bag_of_words/)

- 전처리 후 데이터는 아래와 같다.  

```python
# 결과
[['human', 'interface', 'computer'],
 ['survey', 'user', 'computer', 'system', 'response', 'time'],
 ['eps', 'user', 'interface', 'system'],
 ['system', 'human', 'system', 'eps'],
 ['user', 'response', 'time'],
 ['trees'],
 ['graph', 'trees'],
 ['graph', 'minors', 'trees'],
 ['graph', 'minors', 'survey']]
```

### TF-IDF 손코딩  

#### IDF 계산  

- 9개 문서 중 `system` 이라는 단어가 등장하는 문서는 3개이다.  
- 따라서 `system` 이라는 단어의 IDF 는 `log(9/3) = log3` 이 된다.  
- 표준적으로는 위와 같이 계산하지만, gensim 라이브러리에서는 0을 방지하기 위해 IDF 값에 1을 더해준다.  

#### TF 계산  

- 4번째 문서는 4개의 단어를 가지고 있고, system이라는 단어는 2회 등장한다.  
- 따라서 4번 문장에서 `system` 단어의 TF 값은 `2/4 = 0.5` 가 된다.  

#### TF-IDF 계산  

- TF와 IDF의 곱이므로 `0.5 * log3` 이 TF-IDF 값이 된다.  

### 설치  

- gensim 라이브러리를 설치한다.  
- gensim의 TF-IDF 모델은 BoW와 사전(Dictionary) 기반으로 학습을 수행한다.  

```bash
pip install gensim
```

### 코드  

```python
# 생략
pprint.pprint(processed_corpus) # 샘플 문장들을 전처리 한 결과 리스트
>>>
[['human', 'interface', 'computer'],
 ['survey', 'user', 'computer', 'system', 'response', 'time'],
 ['eps', 'user', 'interface', 'system'],
 ['system', 'human', 'system', 'eps'],
 ['user', 'response', 'time'],
 ['trees'],
 ['graph', 'trees'],
 ['graph', 'minors', 'trees'],
 ['graph', 'minors', 'survey']]

bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus] # BoW
ppritn.pprint(bow_corpus)
>>>
[[(0, 1), (1, 1), (2, 1)],
 [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],
 [(2, 1), (5, 1), (7, 1), (8, 1)],
 [(1, 1), (5, 2), (8, 1)],
 [(3, 1), (6, 1), (7, 1)],
 [(9, 1)],
 [(9, 1), (10, 1)],
 [(9, 1), (10, 1), (11, 1)],
 [(4, 1), (10, 1), (11, 1)]]
```

```python
from gensim import models

# train the model
tfidf = models.TfidfModel(bow_corpus)

# 문서4 TF-IDF 계산
print(tfidf[dictionary.doc2bow(processed_corpus[3])])

# transform the "system minors" string
words = "system minors".lower().split()
print(tfidf[dictionary.doc2bow(words)])
```