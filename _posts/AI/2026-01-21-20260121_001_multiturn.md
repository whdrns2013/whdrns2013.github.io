---
title: "[LLM] 싱글턴과 멀티턴" # 제목 (필수)
excerpt: "LLM이 이전 대화내역을 기억하게 하기" # 서브 타이틀이자 meta description (필수)
date: 2026-01-21 19:45:00 +0900      # 작성일 (필수)
lastmod: 2026-01-21 19:45:00 +0900   # 최종 수정일 (필수)
last_modified_at: 2026-01-21 19:45:00 +0900   # 최종 수정일 (필수)
categories: AI        # 다수 카테고리에 포함 가능 (필수)
tags: llm ai 싱글턴 멀티턴 맥락 single multi turn context singleturn multiturn               # 태그 복수개 가능 (필수)
classes: wide        # wide : 넓은 레이아웃 / 빈칸 : 기본 //// wide 시에는 sticky toc 불가
toc: true        # 목차 표시 여부
toc_label:       # toc 제목
toc_sticky: true # 이동하는 목차 표시 여부 (toc:true 필요) // wide 시에는 sticky toc 불가
header: 
  image:         # 헤더 이미지 (asset내 혹은 url)
  teaser:        # 티저 이미지??
  overlay_image: /assets/images/banners/banner.gif            # 헤더 이미지 (제목과 겹치게)
  # overlay_color: '#333'            # 헤더 배경색 (제목과 겹치게) #333 : 짙은 회색 (필수)
  video:
    id:                      # 영상 ID (URL 뒷부분)
    provider:                # youtube, vimeo 등
sitemap :                    # 구글 크롤링
  changefreq : daily         # 구글 크롤링
  priority : 1.0             # 구글 크롤링
author: # 주인 외 작성자 표기 필요시
permalink: 
sidebar:
  nav: docs_llm
pinned: 
---
<!--postNo: 20260121_001-->

## 멀티턴

### 개념

- 멀티턴 (Multi-turn)  
- 여러 번의 질문과 응답이 서로 이어지며 맥락을 유지하는 상호작용 방식  
- 턴(Turn) : 한 번의 입력과 그에 대한 출력의 쌍  
- 이전 턴들의 대화 이력, 맥락, 의도, 정보를 기억하고 참조하여 답변을 수행하는 것  

### 싱글턴

- 멀티턴과 반대로, 이전 턴들의 대화 이력이나 맥락, 정보를 기억하지 않는 상호작용 방식  
- 매 질문이 독립적이며, 이전 대화를 고려하지 않는다.  

```python
def single_turn():
    while True:
        user_input = input("사용자 입력 : ")
        if user_input == "exit":
            break
        response = chat_gpt(user_message=user_input)
        print("GPT 답변 : " + response.choices[0].message.content)
```

```bash
# 대화 내용
😊 사용자 입력 : 안녕하세요. 저는 Jongya 라고 합니다.
🤖 GPT 답변 : 안녕하세요, Jongya님! 만나서 반갑습니다. 어떻게 도와드릴까요?
😊 사용자 입력 : 제가 누구라고요?
🤖 GPT 답변 : 죄송하지만, 귀하의 신원을 알 수 있는 정보는 제공받지 않았습니다. 어떻게 도와드릴까요?
```

> 이전 대화 내역에서 나온 이름을 기억하지 못한다.  
> 

### 멀티턴

- 멀티턴 구현 방법 : 이전 대화 맥락을 다음 프롬프트에 함께 넣어, LLM이 맥락을 파악할 수 있도록 함  
- OpenAI 의 chatGPT API를 기준으로는 아래와 같이 구현할 수 있다.  

```python
def multi_turn():
    messages = []
    while True:
        # 사용자의 발화 입력
        user_input = input("사용자 입력 : ")
        if user_input == "exit":
            break
        messages.append(OpenAIMessage(role="user", content=user_input))
        # LLM의 답변
        response = chat_gpt(messages=messages).choices[0].message.content
        # LLM의 답변을 messages 에 누적하여 담는다. -> 과거 대화이력 누적
        messages.append(OpenAIMessage(role="assistant", content=response))
        print("GPT 답변 : " + response)
```

```bash
# 대화 내용
😊 사용자 입력 : 안녕하세요. 저는 Jongya 압니다.
🤖 GPT 답변 : 안녕하세요, Jongya님! 만나서 반갑습니다. 어떻게 도와드릴까요?
😊 사용자 입력 : 제가 누구라고요?
🤖 GPT 답변 : 당신은 Jongya라고 소개하셨습니다. 맞나요?
```

> 이전 대화이력에서 “사용자의 이름” 맥락을 기억한다.  
> 

### 유추

- 단순한 기억 말고도, 대화의 맥락에서 사용자가 의도하는 바도 유추할 수 있다.  

```bash
# 멀티턴 적용시
😊 사용자 입력 : 한국의  수도는 어디야?
🤖 GPT 답변 : 한국의 수도는 서울입니다.
😊 사용자 입력 : 미국은?
🤖 GPT 답변 : 미국의 수도는 워싱턴 D.C.입니다.
```

- 싱글턴을 적용해보면  

```bash
# 싱글턴 적용시
😊 사용자 입력 : 한국의 수도가 어디야?
🤖 GPT 답변 : 한국의 수도는 서울입니다.
😊 사용자 입력 : 미국은?
🤖 GPT 답변 : 미국에 대해 알고 싶은 특정한 정보가 있나요? 역사, 문화, 경제, 정치 등 다양한 주제가 있습니다. 궁금한 점을 말씀해 주시면 
자세히 설명해 드리겠습니다.
```



## Reference  

[Do it! LLM을 활용한 AI 에이전트 개발 입문](https://search.shopping.naver.com/book/catalog/54509126926?cat_id=50010921)  