---
title: "[LLM] 주목받는 초 경량 LLM Bitnet"  # 제목 (필수)
excerpt: 1.58비트 양자화 모델 # 서브 타이틀이자 meta description (필수)
date: 2025-05-08 12:45:00 +0900      # 작성일 (필수)
lastmod: 2025-05-08 12:45:00 +0900   # 최종 수정일 (필수)
last_modified_at: 2025-05-08 12:45:00 +0900   # 최종 수정일 (필수)
categories: AI         # 다수 카테고리에 포함 가능 (필수)
tags: AI 에이아이 LLM 언어모델 bitnet                     # 태그 복수개 가능 (필수)
classes: wide        # wide : 넓은 레이아웃 / 빈칸 : 기본 //// wide 시에는 sticky toc 불가
toc: true        # 목차 표시 여부
toc_label:       # toc 제목
toc_sticky: true # 이동하는 목차 표시 여부 (toc:true 필요) // wide 시에는 sticky toc 불가
header: 
  image:         # 헤더 이미지 (asset내 혹은 url)
  teaser:        # 티저 이미지??
  overlay_image: /assets/images/banners/banner.png            # 헤더 이미지 (제목과 겹치게)
  # overlay_color: '#333'            # 헤더 배경색 (제목과 겹치게) #333 : 짙은 회색 (필수)
  video:
    id:                      # 영상 ID (URL 뒷부분)
    provider:                # youtube, vimeo 등
sitemap :                    # 구글 크롤링
  changefreq : daily         # 구글 크롤링
  priority : 1.0             # 구글 크롤링
author: # 주인 외 작성자 표기 필요시
---
<!--postNo: 20250508_001-->

## BitNet  

### BitNet  

[Huggingface - BitNet](https://huggingface.co/microsoft/bitnet-b1.58-2B-4T)    

Microsoft 에서 개발한 **`1.58비트 양자화` 가 적용된 초 경량 LLM 모델 시리즈**. 기존의 LLM 들이 16비트 혹은 8비트 양자화를 한 데 비해 1.58비트 양자화로 엄청난 경량화를 성공한 모델입니다. 여기에 더해 경량임에도 불구하고 **동일 파라미터 수의 full precision 모델과 거의 동일한 성능**을 유지한다는 특징을 보이고 있습니다.  

기존의 LLM 들은 그 모델의 크기와 연산량으로 인해 Local 환경에서 구동이 어려웠지만, BitNet 의 경우 초경량이라는 특징에 따라 Local **기기에서 독립적으로 실행할 수 있는 장점**을 가지게 되었습니다.  


### BitNet 특징 요약

| 구분    | 내용                                                                                                 |
| ----- | -------------------------------------------------------------------------------------------------- |
| 모델 종류 | Transformer 기반 대규모 언어 모델 (LLM)                                                                     |
| 주요 특징 | - 1.58 비트 양자화<br>- 매우 가벼운 모델 (로컬 기기에서 실행 가능할 정도)<br>- 작은 모델 크기 (적은 용량)<br>- 빠른 추론 속도<br>- 적은 전력 소비 |
| 활용 목적 | - 모바일, 엣지컴퓨터, 로컬 기기에서 LLM 구동                                                                       |



### 1.58bit 양자화  

**기존의 LLM은 8~16 비트 스케일의 정밀도**를 가져가기 때문에, 학습 및 추론 과정에서의 연산에 **많은 메모리와 연산량**을 요구하게 됩니다.  

**양자화 모델은 정밀도를 더 낮게** 가져감으로써 모델의 크기와 연산에 필요한 **메모리를 줄인** 모델을 의미합니다. 그리고 BitNet은 그 정밀도를 1.58bit 까지 낮춤으로써 초 경량 LLM 모델이 되었습니다. 

모델의 정밀도를 낮게 하면 모델의 크기가 작아지고(필요 용량이 적어지고), 연산에 필요한 메모리가 적고, 연산량이 적어 동일 사양의 머신에서 **추론 속도가 빨라지고, 전력 소비가 줄어든다는 장점**이 있습니다.  

반면 <u>정밀도가 낮아진 만큼 기존의 8비트, 16비트 정밀도 모델 (full-precision 모델이라고 함)에 비해 낮은 답변 정확도와 낮은 답변 품질을 보이게 되는 단점</u>이 있습니다.  


### Lossless  

BitNet 의 설명을 보면 **Lossless** 라는 단어가 나오는데요, 직역하면 **"무손실의"** 정도가 될 것입니다.  

보통 양자화 모델은 정밀도를 낮게 가져감으로써, **정답의 `성능(정확도, 답변품질) 있어서의 손실` 이 생기게** 됩니다. 쉽게 말해 동일한 파라미터 수를 가진 고 정밀도의 full-precision 모델이 기대 답변의 100%를 맞게 추론한다면, 양자화 모델은 보통 이보다 못한 성능을 보이게 되는 게 일반적입니다. 

하지만 BitNet 은 1.58 비트라는 **굉장히 정밀도가 낮은 모델임에도 불구하고 답변의 출력 품질이 full-precision 모델과 거의 유사**한 정도로 나오고 있습니다. 경량임에 비해 성능이 높다는 것입니다.  

물론, 경량 모델임에 비해 성능이 높다는 것이지, 절대적인 성능에서는 대형 언어모델 보다는 훨씬 낮은 성능을 보여줍니다.  


## BitNet.cpp  

### BitNet.cpp  

[BitNet Github](https://github.com/microsoft/BitNet?tab=readme-ov-file)  

1비트 대의 양자화 LLM 을 효율적으로 구동하는 **인퍼런스(추론) 프레임워크**입니다.   

`.cpp` 확장자를 보면 알 수 있듯 C++ 기반으로 구축되었습니다.  

기존의 LLM 프레임워크는 16비트, 8비트 수준의 LLM 에 최적화되어있기 때문에, 이번에 화제가 되고 있는 BitNet 1.58 비트 양자화 모델을 서빙하기에는 비효율적입니다.  

때문에 저 정밀도의 양자화 모델을 **효율적으로 서빙하기 위해 만들어진 것이 바로 BitNet.cpp** 이며, 아래와 같은 특징을 가지고 있습니다.  

- x86, ARM CPU 기반에서 동작 가능  
- x86 CPU 에서 llama.cpp 에 비해 최대 6.17배 속도 향상, 71.9 ~ 82.2% 에너지 절감  
- ARM CPU 에서 llama.cpp 에 비해 최대 5.07배 속도 향상, 55.4~70.0% 에너지 절감  
- lossless inference 가 가능하도록 지원  


### Inference Framework  

학습된 모델을 실제로 실행하여 입력에 대한 출력을 생성하는 시스템 또는 도구 집합입니다. 쉽게 말해, 학습이 끝난 후 모델을 "사용"할 때 어떤 방식으로 실행할지를 정의하고 지원하는 프레임워크.  

LLM(Large Language Model)은 크기가 크고 복잡해서, 단순한 코드 실행만으로는 효율적으로 사용할 수 없습니다. 그래서 Inference Framework는 다음과 같은 기능들을 제공합니다:  

1. **모델 최적화 (Optimization)**: 빠르고 적은 자원으로 실행할 수 있도록 모델 구조를 변환하거나 경량화합니다.

2. **하드웨어 가속 지원**: GPU, TPU, CPU 등 다양한 하드웨어에서 최적화된 방식으로 실행할 수 있게 해줍니다.
   
3. **배치 처리**: 여러 요청을 한 번에 처리해서 효율을 높입니다.

4. **분산 추론 (Distributed Inference)**: 모델을 여러 서버에 나눠서 실행하거나, 병렬로 처리할 수 있게 합니다.
   
5. **동적 입력 처리**: 다양한 길이와 유형의 입력을 효율적으로 다루기 위한 로직을 포함합니다.

대표적인 Inference Framework 로는 

- **Hugging Face Transformers + Accelerate / Optimum**  
- **TensorRT / ONNX Runtime**: NVIDIA 기반 최적화  
- **vLLM**: 매우 빠른 텍스트 생성 특화  
- **DeepSpeed-Inference**: 마이크로소프트에서 만든 고속 추론 지원  
- **FasterTransformer**: NVIDIA의 고성능 추론용 라이브러리  

## BitNet 사용해보기  

### 설치 및 실행  

[https://github.com/microsoft/BitNet](https://github.com/microsoft/BitNet)  

### Fine Tunning

[https://huggingface.co/blog/1_58_llm_extreme_quantization](https://huggingface.co/blog/1_58_llm_extreme_quantization)

## Reference  

https://github.com/microsoft/BitNet  
[https://huggingface.co/blog/1_58_llm_extreme_quantization](https://huggingface.co/blog/1_58_llm_extreme_quantization)